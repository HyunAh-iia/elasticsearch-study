# 데이터 모델링
- 매핑은 색인될 문서의 데이터 모델링이라 할 수 있음
- 매핑을 설정해 두지 않으면 엘라스틱서치가 자동으로 필드를 생성하고 필드 타입까지 결정함
---
## 1. 매핑 API 이해하기
- 필드의 속성을 정의할 때 각 필드 속성에는 데이터 타입과 메타데이터가 포함됨
- 색인 과정에서 문서가 어떻게 역색인(Inverted Index)으로 변환되는지를 상세히 정의 가능
- 한번 정의된 필드에 서로 다른 타입의 데이터가 입력된다면 뒤에 입력된 데이터의 색인 생성 실패
- 생성된 매핑의 타입은 변경 불가 (인덱스 재생성해야함)
- 매핑 정보를 설정할 때의 고려사항
    - 문자열을 분석할 것인가?
    - _source에 어떤 필드를 정의할 것인가?
    - 날짜 필드를 가지는 필드는 무엇인가?
    - 매핑에 정의되지 않고 유입되는 필드는 어떻게 처리할 것인가?
   
### 매핑 인덱스 만들기
- 인덱스 생성
    - ```
      PUT 인덱스명
      {
        "setting": {
            ...
        },
        "mappings": {
          "_doc": {
            "properties": {
                "컬럼명": {
                  "type": "keyword"
                },
                "movieNm": {
                  "type": "text",
                  "analyzer": "standard"
                },
                "companies": {
                  "properties": {
                    "companyCd": { "type": "keyword" },
                    "companyNm": { "type": "keyword" }
                  }
                }
            }
          }
        }
      }
      ```
- 인덱스 생성 결과
    - ```
      {
        "acknowleged": true,
        "shard_acknowledged": true,
        "index": "인덱스명"
      }
      ```

### 매핑 확인
- _mapping API를 통해 이미 만들어진 매핑 정보를 확인 가능
- ```
  GET 인덱스명/_mapping
  ```

### 매핑 파라미터
- 색인할 필드의 데이터를 어떻게 저장할지에 대한 다양한 옵션을 제공함
1. analyzer
    - 해당 필드의 데이터를 형태소 분석하겠다는 의미
    - 색인과 검색 시 지정한 분석기로 형태소 분석을 수행
    - text 데이터 타입의 필드는 analyzer 매핑 파라미터를 기본적으로 사용해야 함
    - 별도의 분석기 미지정 시 Standard Analyzer로 형태소 분석
2. normalizer
    - term query 분석기를 사용하기 위해 쓰임
    - keyword 데이터 타입의 경우 원문을 기준으로 문서가 색인되기 때문에 cafe, Cafe, CAFE는 서로 다른 문서로 인식됨
    - 하지만 해당 유형을 normalizer를 통해 분석normalizer기에 asciifolding과 같은 필터를 사용하면 같은 데이터로 인식 가능
3. boost
    - 필드에 가중치(Weight)를 부여
    - 가중치에 따라 유사도 점수(_score)가 달라짐
    - boost 설정 시 검색 결과의 노출 순서에 영향을 줌
    - 만약 색인 시점에 boost 설정하게 된다면 재색인을 하지 않는 이상 가중치 변경 불가
    - 가급적이면 검색 시에만 사용하는 것을 권장
    - 최신 엘라스틱서치는 boost 설정을 할 수 없도록 바꿈
4. coerce
    - 색인 시 자동 변환을 허용할지 여부를 설정하는 파라미터
    - 엘라스틱서치는 자동을 "10"을 숫자 10으로 형변환하여 수행해서 정상적`으로 처리한다
    - 하지만 coerce 설정을 미사용으로 하면 색인에 실패할 것임
5. copy_to
    - 사라진 _all 기능 대신 활용 가능 (여러 개의 필드를 하나로 모아 검색 가능함)
    - 매핑 파라미터를 추가한 필드의 값을 지정한 필드로 복사
    - ex) keyword 타입의 필드에 copy_to 매핑 파라미터를 사용해 다른 필드로 값을 복사하면, 복사된 필드에서는 text 타입을 지정해 형태소 분석 가능
6. fielddata
    - 힙 공간에 생성하는 메모리 캐시
    - 메모리 부족 현상과 잦은 GC로 현재는 거의 사용되지 않음
    - 최신버전의 엘라스틱서치는 doc_value라는 새로운 형태의 캐시를 제공하고 있다
    - fielddata를 사용해야하는 경우가 있음
        - text 타입의 필드는 기본적으로 분석기에 의해 형태소 분석이 되기 때문에 집계나 정렬 기능을 수행할 수 없음
        - text 타입의 필드에서 집계나 정렬을 수행하고자 할 때에 한해 fielddata를 사용할 수 있음
        - 그러나 fielddata는 메모리에 생성되는 캐시이기 때문에 최소한만으로 사용해야함
    - 메모리 소모가 크기 때문에 기본적으로 비활성화 되어있음
7. doc_values
    - 엘라스틱서치에서 사용하는 기본 캐시
    - text 타입의 필드를 제외한 모든 필드는 기본적으로 doc_value 캐시를 사용함
    - doc_values를 사용함으로써 힙 사용에 대한 부담을 없애고 운영체제의 파일 시스템 캐시를 통해 디스크에 있는 데이터에 빠르게 접근 가능
    - 필드를 정렬, 집계할 필요가 없고 스크리브에서 필드 값에 액세스할 필요가 없다면 디스크 공간을 절약하기 위해 doc_values를 비활성화할 수 있음
    - 한 번 비활성화된 필드는 인덱스를 재색인하지 않는 이상 변경 불가
8. dynamic
    - 매핑에 필드를 추가할 때 동적으로 생성할지, 생성하지 않을지를 결정함
    - 동정 생성 필드으 처리 방법으로 세 가지가 있음
        - true : 새로 추가되는 필드를 매핑에 추가함
        - false : 새로 추가되는 필드를 무시함. 해당 필드는 색인이 되지 않아 검색할 수는 없지만 _source에 표시됨
        - strict : 새로운 필드가 감지되면 예외가 발생하고 문서 자체가 색인되지 않음. 새로 유입되는 필드는 사용자가 매핑에 명시적으로 추가해야함
9. enabled
    - 검색 결과에는 포함하지만 색인은 하고 싶지 않은 경우 사용
10. format
    - 엘라스틱서치는 날짜/시간을 문자열로 표현
    - 정해진 표현 포맷이 있음
11. ignore_above
    - 필드에 저장되는 문자열이 지정한 크기를 넘어서면 빈 값으로 색인함
    - 중간에 잘리는 게 아니라, 빈 값으로 저장됨을 주의
12. ignore_malformed
    - 엘라스틱서치에서는 잘못된 데이터 타입을 색인하려고 하면 예외가 발생하고 해당 문서 전체가 색인되지 않음
    - 이 매핑 파라미터를 통해 해당 필드만 무시하고 문서 색인 가능
13. index
    - 필드값을 색인할지를 결정함
14. fields
    - 다중 필드(multi_field)를 설정할 수 있는 옵션
    - 필드 안에 또 다른 필드의 정보를 추가할 수 있어 같은 string 값을 각각 다른 분석기로 처리할 수 있다.
15. norms
    - 문서의 _score 값 계싼에 필요한 정규화 인수를 사용할지 여부를 설정
    - _scoure계산이 필요없거나 단순 필터링 용도로 사용하는 필드는 비활성화해서 디스크 공간을 절약할 수 있음
16. null_value
    - 엘라스틱서치는 색인 시 문서에 필드가 없거나 필드의 값이 null이면 색인 시 필드를 생성하지 않음
    - 해당 파라미터를 설정하면 문서의 값이 null이더라도 필드를 생성하고 그에 해당하는 값으로 저장함(null value에 대한 default값 지정가능)
17. position_increment_gap
    - 배열 형태의 데이터를 색인할 때 검색의 정확도를 높이기 위해 사용하는 옵션
    - 필드 데이터 중 단어와 단어 사이의 간격(slop)을 허용할지를 설정함
    - 검색 시 단어와 단어 사이의 간격을 기준으로 일치하는 문서를 찾는 데 필요
    - ex) ["john Abraham", "Lincon Smith"]일 때, "Abraham Lincon"으로 검색하더라도 검색이 가능
18. properties
    - Object 타입이나 중첩(Nested) 타입의 스키마를 정의할 때 사용되는 옵션
19. search_analyzer
    - 일반적으로 색인과 검색 시 같은 분석기를 사용함
    - 그러나 다른 분석기를 사용하고 싶은 경우 해당 옵션을 통해 별도 지정 가능
20. similarity
    - 유사도 알고리즘을 지정
    - 엘라스틱서치에서 미리 정의된 유사도 측정 알고리즘은 다음과 같다
        - BM25 : Okapi BM25 알고리즘. 엘라스틱서치의 기본 유사도 측정 알고리즘
        - classic : TF/IDF 알고리즘. 문서 내 용어의 개수와 전체 용어의 개수를 이용해 유사도 계산
        - boolean : 복잡한 수학적 모델을 사용하지 않고 단순히 boolean 연산으로 유사도 측정
21. store
    - 필드의 값을 저장해 검색 결과에 값을 포함하기 위핸 매핑 파라미터
22. term_vector
    - 분석된 용어의 정보를 포함할지 여부를 결정하는 옵션
    - 다음과 같은 인자가 있음
        - no : 텀벡터를 저장하지 않음
        - yes : 필드와 용어만 저장
        - with_positions : 용어, 용어의 시작과 끝 위치를 저장
        - with_offsets : 용어, 문자 오프셋을 저장
        - with_positions_offsets : 용어, 용어의 시작과 끝 위치, 문자 오프셋을 모두 저장
---
## 2. 메타 필드
메타데이터를 저장하는 특수 목적의 필드로, 이를 이용하면 검색 시 문서를 다양한 형태로 제어하는 것이 가능함

### _index 메타 필드
- 해당 문서가 속한 인덱스의 이름을 담고 있음
- _index 컬럼으로 집계할 경우, 인덱스별 총 건수를 확인할 수 있음

### _type 메타 필드
- 문서가 속한 매핑의 타입 정보를 담고 있음

### _id 메타 필드
- 문서를 식별하는 유일한 키 값

### _uid 메타 필드
- 특수한 목적의 식별키로, "#" 태그를 사용해 _type과 _id값을 조합함
- 내부적으로만 사용되기 때문에 검색 시 조회되는 값은 아님

### _source 메타 필드
- 문서의 원본 데이터를 제공함
- ex) 재색인 시, 특정 컬럼의 값에 접근하는 표기법
    - ```
      "script": {
        "source": "ctx._source.컬럼명++" #컬럼에 1을 더함
      }
      ```
      
### _all 메타 필드
- 엘라스틱서치 6.0 이후 폐기됨
- copy_to 메핑 파라미터 활용하면 _all과 동일한 기능 가능

### _routing 메타 필드
- 특정 문서를 특정 샤드에 저장하기 위해 사용자가 지정하는 메타필드
- 검색할 때도 색인할 때와 마찬가지로 _routing 값을 지정해야함
---
## 3. 필드 데이터 타입

### Keyword 데이터 타입
- 키워드 형태로 사용할 데이터에 적합한 데이터 타입
- 별도의 분석기를 거치지 않고 원문 그대로 색인
- 정형화된 콘텐츠에 주로 사용됨
- 일부 기능은 형태소 분석을 하지 않아야만 사용 가능한데, 이 경우에도 Keyword 데이터 타입을 사용함
- Keyword 타입을 사용해야하는 경우
    - 검색 시 필터링되는 항목
    - 정렬이 필요한 항목
    - 집계(Aggregation)해야 하는 항목
- Keyword 타입은 색인 시 대소문자 구분함. 하지만 매핑 파라미터의 normalizer를 통해 캐릭터필터와 일부 토큰필터 적용 가능
- 주요 파라미터
    - boost
    - doc_values
    - index
    - null_value
    - store
        
### Text 데이터 타입
- 색인 시 지정된 분석기가 컬럼의 데이터를 문자열 데이터로 인식하고 이를 분석함
- 문장 형태의 데이터에 적합한 타입임
- 전문 검색 가능
- 정렬이나 집계 연산을 사용해야할 때, Text 타입과 Keyword 타입을 동시에 갖도록 멀티 필드로 설정 가능
- 주요 파라미터
    - analyzer
    - boost
    - fielddata
    - index
    - norms
        - 매핑파라미터 p 73과 설명이 다름
        - 매핑파라미터 설명(p73) : norms : 문서의 _score 값 계산에 필요한 정규화 인수를 사용할지 여부. _score 계산이 필요없거나 단순 필터링 용도로 사용 시 비활성화해서 디스크 공간 절약 가능
        - Text 매핑파라미터 : 유사도 점수를 산정할 때 필드 길이를 고려할지를 결정한다. 기본값 true임
    - store
    - search_analyzer
    - similarity
    - term_vector : Analyzed 필드에 텀벡터를 저장할지 결정. 기본값은 no
    
### Array 데이터 타입
- 모든 필드는 기본적으로 멀티값을 가질 수 있음
- 때문에 매핑 시 명시적으로 Array 타입을 정의하지 않음
- 정의된 인덱스 필드에 단순히 배열 값을 입력하면 자동으로 Array 형태로 저장됨
- 정의된 값들은 모두 같은 타입으로만 구성되어야 함
- Array 타입은 문자열이나 숫자, 객체 형태로 정의 가능함
- 중요 : 데이터가 배열 형태로 저장되면 한 필드 내의 검색은 기본적으로 OR 조건임
- 이러한 특성 탓에 저장되는 데이터 구조가 복잡해지면 모호한 상황이 일어날 수 있음

### Numeric 데이터 타입
- 엘라스틱서치에서 숫자 데이터 타입은 여러 가지 종류가 제공됨

### Date 데이터 타입
- Date 타입은 JSON 포맷에서 문자열로 처리됨
- 올바른 구문 분석을 위해 날짜 문자열을 형식을 명시적으로 설정해야 함
- 기본 형식 : "yyyy-MM-ddTHH:mm:ssZ"

### Range 데이터 타입
- 범위가 있는 데이터를 저장할 때 사용
- 날짜, 정수, 실수, IP 등 다양한 타입의 범위 지원

### Boolean 데이터 타입
- 참과 거짓이라는 두 논리값을 가지는 데이터 타입
- true, "true", false, "false"로 표현 가능

### Geo-Point 데이터 타입
- 위도, 경도 등 위치 정보를 담은 데이터

### IP 데이터 타입
- IP 주소와 같은 데이터를 저장하는 데 사용됨
- IPv4, IPv6 모두 지정 가능

### Object 데이터 타입
- JSON 포맷의 문서는 내부 객체를 계층적으로 포함 가능함
- Object 데이터 타입 정의 시 다른 데이터 타입과 같이 특정 키워드를 사용하지 않음
- 단지 필드값으로 다른 문서의 구조를 입력하면 됨
- ex) '코로나확진자동선'이라는 인덱스를 생성해보자(인덱스명은 영소문자여야함)
- ```
  PUT 코로나확진자동선/_mapping/_doc
  {
    "properties": {
      "이름": {
        "type": "text"
      },
      "이동경로": {
        "proferties": {
          "행정동": {
            "type": "text"
          },
          "방문지": {
            "type": "text"
          },
          "방문일시": {
            "type": "date",
            "format": "yyyy-MM-dd HH:mm:ss"
          }
      }
    }
  }
  ```
- ex) 아래와 같이 문서를 저장해보자
- ```
  PUT 코로나확진자동선/_doc/10
  {
    "이름": "김꾸러기",
    "이동경로": [
      {
        "행정동" : "신도림동",
        "방문지" : "김밥랜드 신도림점",
        "방문일시" : "2020-04-07 19:30:00"
      },
      {
        "행정동" : "화곡동",
        "방문지" : "해피마트 까치산점",
        "방문일시" : "2020-04-11 21:30:00"
      }
    ]
  }
  ```
- 자! 그럼 데이터를 조회해보자
    1. 검색조건 : 행정동이 "신도림동"이고, 방문일시가 "2020-04-07 ~"
        - 검색결과 : "김꾸러기"가 조회됨
    2. 검색조건 : 행정동이 "신도림동"이고, 방문일시가 "2020-04-11 ~"
        - 검색결과 : "김꾸러기"가 조회됨
        - 김꾸러기는 "2020-04-11"일에 "신도림동"에 가지 않았음
        - 하지만 Object 내부 연산은 OR 연산으로 검색이 되었음ㅠㅠ
    
- Object 내 중첩필드에 대한 OR 연산이 이뤄진다.
- 이러한 문제를 해결하기 위해 nested 데이터 타입이 고안됨

### Nested 데이터 타입
- Nested 데이터 타입은 Object 객체 배열을 독립적으로 색인하고 질의하는 형태의 데이터 타입임
- 이를 사용하면 중첩된 필드를 검색할 때 정확히 일치하는 문서만 출력 가능함
- 동일한 예제로 조회를 해보자!
    1. 검색조건 : 행정동이 "신도림동"이고, 방문일시가 "2020-04-07 ~"
        - 검색결과 : "김꾸러기"가 조회됨
    2. 검색조건 : 행정동이 "신도림동"이고, 방문일시가 "2020-04-11 ~"
        - 검색결과 : "김꾸러기"가 조회되지 않음
---
## 4. 엘라스틱서치 분석기
- 1주차에 정리해놓은 글과 함께 보면 도움이 된다.
    > elasticsearch-study\01. 검색 시스템 이해하기\김현아\역인덱스와 Text Analyser.md 
- Elastic 공식 가이드북  (김종민님 번역)
    > <https://esbook.kimjmin.net/06-text-analysis>

### 텍스트 분석 개요
- 엘라스틱서치는 텍스트를 처리하기 위해 기본적으로 분석기를 사용하는 데, 우리가 기대와는 조금 다를 수 있다
- 텍스트가 분석되면 개별 텀으로 나뉘어 형태소 형태로 분석된다
- 해당 형태소는 특정 원칙에 의해 필터링되어 단어가 삭제되거나 추가, 수정되고 최종적으로 역색인된다.
- 이러한 방식의 텍스트 분석은 언어별로 조금씩 다르게 동작한다 (한글에 맞게 분석해야함)

### 역색인 구조
- 역색인 구조
    - 모든 문서가 가지는 단어의 고유 단어 목록
    - 해당 단어가 어떤 문서에 속해 있는 지에 대한 정보
    - 전체 문서에 각 단어가 몇 개 들어있는 지에 대한 정보
    - 하나의 문서에 단어가 몇 번씩 출현했는지에 대한 빈도
- 색인한다 = 역색인 파일을 만든다
- 분석(Analuze) : 색인할 때 특정한 규칙과 흐름에 의해 텍스트를 변경하는 과정
- 분석기(Analyzer) : 분석은 분석기라는 모듈을 조합해서 처리됨

### 분석기의 구조
- 분석기는 기본적으로 다음과 같은 프로세스로 동작
    <img src="https://gblobscdn.gitbook.com/assets%2F-Ln04DaYZaDjdiR_ZsKo%2F-LntYrdKmTe441TqYAJl%2F-LntZ63SAIfHu6Q_OgzJ%2F6.2-02.png?alt=media&token=52213afe-e6ab-4bc2-b9e0-20027542a79e"></img>
    1. 문장을 특정한 규칙에 의해 수정 (0~3개의 캐릭터필터)
    2. 수정한 문장을 개별 토큰으로 분리 (1개의 토크나이저 필터)
    3. 개별 토큰을 특정한 규칙에 의해 변경 (0~N개의 토큰 필터)
    
- 세 가지 프로세스는 다음과 같은 용어로 불림
    1. Character filter (캐릭터 필터)
        - 텍스트 토큰화 하기 전 전처리 단계
        - 특정 단어를 변경하거나 html과 같은 태그를 제거하는 역할
        - ReplaceAll() 함수처럼 패턴으로 텍스트를 변경하거나 사용자가 정의한 필터 적용 가능
    2. Tokenizer filter (토크나이저 필터)
        - 분석기 구성에서 하나만 사용할 수 있음
        - 텍스트를 어떻게 나눌 것인지를 정의
        - 각 언어에 적절한 형태소 분석기의 Tokenizer를 사용하면 됨
    3. Token filter (토큰 필터)
        - 토큰화된 단어를 하나씩 필터링해서 사용자가 원하는 토큰으로 변환
        - 불용어 처리, 동의어 사전, 대소문자 변환 등의 작업 수행
        - N개의 토큰필터는 순차적으로 적용되니 순서에 유의
- ex
- ```
  # 분석할 문장
  <B>Elasticsearch</B> is cool
  
  # 인덱스 생성 시점에 분석기 정의
  PUT /인덱스명
  {
    "settings": {
      ...
    },
    "analysis": {
      "my_custom_analyzer": {
        "type": "custom",
        "char_filter": [
          "html_strip"
        ],
        "tokenizer": "standard",
        "filter": [
          "lowercase"
        ]
      }
    }
  }
  ```
- 처리되는 과정을 살펴보자
    1. char_filter : html_strip
        - 전체 텍스트에서 HTML 태그를 제거
        - Elasticsearch is cool
    2. tokenizer : standard (루씬에서 제공하는 기본 분석기)
        - 특수문자 혹은 공백을 기준으로 텍스트를 분해
        - Elasticsearch, is, cool
    3. filter : lowercase
        - 모든 토큰을 소문자로 변환
        - elasticsearch, is, cool

### 분석기 사용법
- 분석기를 이용한 분석
    - 엘라스틱서치에서는 형태소가 어떻게 분석되는지 확인할 수 있는 _analyze API를 제공함
    - 미리 정의된 분석기의 경우 이를 이용해 테스트 가능함
    - 다음과 같이 설정하면 특정 분석기를 이용했을 때 어떻게 토큰이 분리되는 지 확인 가능
    - ```
      POST _analyze
      {
        "analyzer": "standard",
        "text": "캐리비안의 해적"
      }
      ```
- 필드를 이용한 분석
    - 앞서 인덱스에 설정한 my_custom_analyzer 분석기를 매핑한 필드를 통해서도 확인이 가능하다
    - ```
      POST 인덱스명/_analyze
      {
        "field": "분석기를 설정한 필드명",
        "text": "확인할 문장"
      }
      ```
- 색인과 검색 시 분석기를 각각 설정
    - 색인과 검색용 분석기를 다르게 적용할 수 있음
    - 필드의 매핑 파라미터를 아래와 같이 설정하면 됨
    - analyzer 파라미터 : 색인용 분석기(Index Analyzer) 지정
    - search_analyzer 파라미터 : 검색용 분석기Search Analyzer) 지정
### 대표적인 기본 분석기
- 엘라스틱서치에서 제공하는 기본 분석기들이 있음
- 이 가운데 많이 사용되는 대표적인 분석기 세 가지를 살펴봄
1. Standard Analyzer
    - 공백 혹은 특수문자를 기준으로 토큰 분리
    - 모든 문자를 소문자로 변경하는 토큰 필터 사용
2. Whitespace 분석기
    - 공백 문자열을 기준으로 토큰 분리
3. Keyword 분석기
    - 토큰화 작업을 하지 않음
    - 전체 입력 문자열을 하나의 키워드처럼 처리

### 전처리 필터 (Character Filter)
> - 책에서는 전처리 필터의 활용도가 높지 않다며 html_strip만 설명함
> - 해당 부분은 공식문서를 참고하여 세 개의 캐릭터필터를 정리함 
> - https://esbook.kimjmin.net/06-text-analysis/6.4-character-filter
1. HTML Strip
    - 입력된 텍스트가 HTML 인 경우 HTML 태그들을 제거하여 일반 텍스트로 변환
    - 사용 예시
    - ```
      POST _analyze
      {
        "tokenizer": "keyword",
        "char_filter": [
          "html_strip"
        ],
        "text": "<p>I&apos;m so <b>happy</b>!</p>"
      }
      ```
    - 애널라이저는 항상 최소 1개의 토크나이저를 필요로 하기 때문에 캐릭터 필터만 적용하면 오류가 발생함
    - 위 예제에서는 keyword 토크나이저를 같이 사용했다고 함
    - 참고 : 특정 태그는 유지하도록 설정도 가능함
2. Mapping
    - 지정한 단어를 다른 단어로 치환이 가능
    - 특수문자 등을 포함하는 검색 기능을 구현하려는 경우 반드시 적용해야 해서 실제로 캐릭터 필터 중에는 가장 많이 쓰임
    - 필요한 상황 예시
        - standard 분석기는 특수기호를 제거함
        - C, C++ ==> 모두 "c"라는 term으로 저장됨
    - 내부 동작
        - "+" 기호를 "_plus"로 치환
        - C++ ==> c_plus_plus
    - C, C++이 각각 "c", "c_plus_plus" 라는 term으로 저장됨
    - 검색 시 C++을 입력하면 동일한 애널라이저가 C++을 c_plus_plus로 치환하여 검색함
3. Pattern Replace
    - 정규식(Regular Expression)을 이용해서 좀더 복잡한 패턴들을 치환

### 토크나이저 필터
- 분석기를 구성하는 가장 핵심 구성요소
- 텍스트를 term으로 분해하는 기준
- 엘라스틱서치에서 제공되는 대표적인 토크나이저 살펴보기
    1. Standard 토크나이저
        - 엘라스틱서치에서 일반적으로 사용하는 토크나이저
        - 대부분의 기호를 만나면 토큰으로 나눔
    2. Whitespace 토크나이저
        - 공백을 만나면 토큰으로 나눔
    3. Ngram 토크나이저 (standard 토크나이저에 Ngram 토큰필터를 적용한 결과)
        - 기본적으로 한 글자씩 토큰화함
        - 특정 문자를 지정할 수도 있음
        - 다양한 옵션을 조합해서 자동완성을 만들 때 유용하게 활용 가능
        - 그러나 텀의 개수가 기하급수적으로 늘어나고, 검색 결과를 예측하기 어려움 (공식문서)
        - 텀의 수가 제한적인 카테고리 목록이나 태그 목록과 같은 데이터 집단에 자동완성 기능을 구현하기 적합함 (공식문서)
        - 공식문서 : https://esbook.kimjmin.net/06-text-analysis/6.6-token-filter/6.6.4-ngram-edge-ngram-shingle
        - 옵션
          ```
          min_gram : 적용할 문자의 최소 길이를 나타냄. 기본값은 1
          max_gram : 적용할 문자의 최대 길이를 나타냄. 기본값은 2
          token_chars : 토큰에 포함할 문자열을 지정함
          token_chars의 옵션 : letter(문자), digit(숫자), whitespace(공백), punctuation(구두점), symbol(특수기호)
          ```
        - ex
          ```
          # Ngram 토크나이저 설정
          min_gram : 3
          max_gram : 3
          token_chars : letters
          
          # 문장 분리 예시
          "Harry Potter"
          ==> [Har, arr, rry, Pot, ott, tte, ter]
          ```
    4. Edge Ngram 토크나이저 (standard 토크나이저에 Edge Ngram 토큰필터를 적용한 결과)
        - 지정된 문자의 목록 중 하나를 만날 때 마다 시작 부분을 고정시켜 단어를 자르는 방식
        - 해당 토크나이저 역시 자동 완성을 구현할 때 유용하게 활용 가능
        - 옵션은 Ngram 토크나이저와 동일함
        - ex
          ```
          # Edge Ngram 토크나이저 설정
          min_gram : 2
          max_gram : 10
          token_chars : letters
          
          # 문장 분리 예시
          "Harry Potter"
          ==> [Ha, Har, Harr, Harry, Po, Pot, Pott, Potte, Potter]
          ```
    5. Keyword 토크나이저
        - 텍스트를 하나의 토큰 만듬

### 토큰 필터
- 토크나이저에 의해 분리된 토큰은 배열 형태로 토큰 필터로 전달됨
- 토큰필터는 전달받은 토큰들을 변형하거나 추가, 삭제할 때 사용함
- 토크나이저에 의해 토큰이 모두 분리돼야 동작하기 때문에 독립적으로 사용 불가
- 주요 토큰 필터
    1. Ascii Folding 토큰 필터
        - 아스키 코드에 해당하는 127개의 알파벳, 숫자, 기호에 해당하지 않는 경우, 문자를 ASCII 요소로 변경함
        - ex) javacafé ==> javacafe
    2. Lowercase 토큰 필터
        - 전체 문자열을 소문자로 변환
    3. Uppercase 토큰 필터
        - 전체 문자열을 대문자로 변환
    4. Stop 토큰 필터
        - 불용어로 등록할 사전을 구축해서 사용하는 필터를 의미
        - 인덱스를 만들고 싶지 않거나 검색되지 않게 하고 싶은 단어를 등록하는 불용어 사전 구축
        - 엘라스틱에서 지원하는 언어별 불용어 사전 : https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stop-tokenfilter.html#analysis-stop-tokenfilter-stop-words-by-lang
        - 한국어는 없다. 한, 중, 일어 등은 별도의 형태소 분석기를 사용해야 함
        - 옵션
          ```
          stopwords : 불용어를 매핑에 직접 등록해서 사용
          stopwords_path : 불용어 사전이 존재하는 경로 지정. 해당 경로는 엘라스틱서치 서버가 있는 config 폴더 안에 생성
          ignore_case : true일 경우 모든 단어를 소문자로 변경해서 저장. 기본값은 false
          ```
            
### 동의어 사전
    - Synonym 토큰필터를 이용하면 동의어 처리가 가능함
    - 동의어는 원문에 특정 단어가 존재하지 않더라도 색인 데이터를 토큰화해서 자장할 때 동의어나 유의어에 해당하는 단어와 함께 저장해서 검색이 가능해지게 하는 기술
    - 동의어 추가 방식
        1. 동의어를 매핑 설정 정보에 미리 파라미터로 등록
        2. 특정 파일을 별도로 생성해서 관리
    - 실무에서는 동의어를 모아둔 파일들을 지칭할 때 "동의어 사전"이라는 용어 사용
1. 동의어 사전
    - 동의어 파일은 엘라스틱서치가 설치된 서버 아래의 config 디렉터리에 생성해야함
    - ```<엘라스틱서치 설치 디렉터리>/config/analysis/synonym.txt```
2. 동의어 추가
    - 동의어 추가할 때 단어를 쉼표(,)로 분리해 등록하는 방법
    - 예를 들어 "Elasticsearch"와 "엘라스틱서치"를 동의어로 지정한다면
    - "Elasticsearch, 엘라스틱서치"로 등록하면 된다.
    - 주의 : 동의어 처리 기준은 앞서 동작한 토큰 필터에 따라 달라질 수 있다.
    - lowercase 토큰필터로 elasticsearch로 들어온 term은 "Elasticsearch, 엘라스틱서치"와 일치하지 않기 때문에 동의어가 적용되지 않는다.
    - 최신 버전 엘라스틱서치는 동의어 처리 시 대소문자를 구분하지 않는음
3. 동의어 치환
    - 특정 단어를 어떤 단어로 변경하고 싶다면 치환 기능을 사용하면 됨
    - "Harry => 해리"로 한다면, "Harry"라는 term은 "해리"로 변경되어 색인됨
4. 동의어 사전의 적용 시점 (!@@)
    - 동의어 사전은 실시간으로 적용되지 않음
    - 인덱스를 reload해야 함
      ```
      POST 인덱스명_close
      POST 인덱스명_open
      ```
    - 이 때 주의할 점은, 동의어 사전은 색인 시점에도 사용될 수 있고 검색 시점에도 사용될 수 있다는 점
    - 검색 시점에는 사전의 내용이 변경되더라도 해당 내용은 반영됨
    - 하지만 색인 시점에는 사전의 내용이 변경되더라도 색인이 변경되지 않음
    - 이 경우 기존 색인을 모두 삭제하고 색인을 다시 생성해야 변경된 사전 내용이 적용됨
    - 이러한 문제점 때문에 동의어 사전이 빈번하게 수정되는 인덱스의 경우 색인 시점에는 적용하지 않고, 검색 시점에만 적용하는 방식으로 해결하기도 함
---
## 5. Document API 이해하기
### 문서 파라미터
1. 문서 ID 자동 생성
    - 각 문서는 ID로 구분함
    - 만약 문서 추가 시 ID를 지정하지 않으면 자동으로 UUID 형태의 ID가 생성됨
2. 버전관리
    - 색인된 모든ㄷ 문서는 버전 값을 가짐
    - Update API를 이용할 경우 내부적으로 스냅숏을 생성해서 문서를 수정하고 인덱스에 다시 재색인하는데, 이때 버전 정보를 이용함
    - 스냅숏이 생성된 사이에 버전 값이 달라졌다면 실패로 처리함
    - 버전 정보가 일치하지 않는 경우 에러를 반환함
    - 버전은 직접 지정할 수도 있는데, 이 떄 입력되는 버전 값은 반드시 정수여야 함
3. 오퍼레이션 타입 (op_type)
    - 엘라스틱서치에서는 이미 ID가 존재하는 경우 update 작업이, ID가 없을 경우 create 작업이 일어난다.
    - 만약 데이터가 존재할 경우 update하지 않고 색인이 실패하길 원한다면 어떻게 할까?
    - Index API 호출 시 op_type 파라미터를 이용하면 수행되는 작업의 유형을 강제로 지정할 수 있음
      ```
      PUT 인덱스명/_doc/문서ID?op_type=create
      {
        ...
      }
      ```
    - 처음 색인 시에는 정상적으로, 이후에는 이미 문서가 존재한다는 에러가 발생하면서 색인에 실패한다
4. 타임아웃 설정
    - 일반적으로 색인 요청 시 대부분 즉시 처리됨
    - 하지만 이미 색인 작업이 진행 중인 동안 색인 API가 추가로 호출된 경우, 즉시 처리되지 못하고 일정 기간 대기하게 됨
    - 기본적으로 1분간 대기하게 되는데, 1분이 지날 경우 요청 자체가 실패함
    - 이 때, timeout 파라미터를 설정해 대기 시간 조절 가능
      ```
      PUT 인덱스명/_doc/문서ID?timeout=5m
      {
        ...
      }
      ```
5. 인덱스 매핑 정보 자동 생성
    - Index API로 문서 색인 시, 기존에 정의되지 않은 필드의 정보가 존재할 경우 어떻게 해야 할지 결정해야 함
    - 기본적으로 동적 매핑을 허용하기 떄문에 색인 즉시 새로운 필드가 생성됨
    - 동적 매핑을 허용할 경우 원치 않는 오류가 발생할 수 있기 때문에 상황에 따라서 해당 기능을 비활성화 해야함
    - elasticsearch.yml에서 아래와 같이 동적 매핑에 대한 설정이 가능함
      ```
      action.auto_create_index : 인덱스 자동 생성 여부를 설정. 기본값 true
      index.mapper.dynamic : 동적 매핑 사용 여부를 설정. 기본값 true
      ```
### index API
- 문서를 특정 인덱스에 추가하는 데에 사용됨
- 문서가 업데이트될 때마다 버전 값이 1씩 증가
- ```
  PUT 인덱스명/_doc/문서ID
  {
    ...
  }
  ```
- 문서 색인 결과
    - _shard 항목은 몇 개의 샤드에서 명령이 수행됐는지에 대한 정보를 나타냄
    - total - 복제돼야 하는 전체 샤드 개수
    - successful - 성공적으로 복제된 샤드 개수
    - Index API는 최소 한개 이상의 successful 항목이 있어야 성공한 것으로 간주

### Get API
- 특정 문서를 인덱스에서 조회할 때 사용하는 API
- 조회하고자 하는 문서의 ID를 명시저그로 지정하여 사용
- ```
  GET 인덱스명/_doc/문서ID
  ```
- 문서 조회 결과
    - 원문 데이터의 모든 필드가 _source 필드의 일부분으로 저장됨
    - 데이터 크기가 너무 방대해 특정 필드를 _source 항목으로 제공되지 않도록 설정 가능
    - _source_exclude 옵션을 이용해 제외할 필드명을 지정할 수 있음
    - ```
      GET 인덱스명/_doc/문서ID?_source_exclude=제외할필드명
      ```
### Delete API
- 문서 삭제할 때 사용하는 API
- 삭제 시, result 하옴ㄱ에 "deleted"가 반환됨
- version 값이 1만큼 증가한 것을 확인할 수 있음
- ```
  DELETE 인덱스명/_doc/문서ID
  ```
### Delete By Query API
- 특정 인덱스에서 검색 결과에 맞는 문서만 삭제하고 싶은 경우 사용하는 API
- ```
  POST 인덱스명/_doc/문서ID
  {
    ... query ...
  }
  ```
- 검색 결과
    - 몇 건이 조회되었고, 몇 건의 데이터가 삭제됐는 지 확인 가능
    - deleted : 삭제된 문서 수
    - version_conflicts : 버전 불일치로 실패한 문서 수
### Update API
- 스크립트를 바탕으로 문서를 수정할 수 있음
- 필드값은 "ctx._source.필드명" 형태로 접근 가능
- ```
  POST 인덱스명/_doc/문서ID/_update
  {
    "script" : {
        "source": "ctx._source.counter += params.count",
        "lang" : "painless",
        "params" : {
            "count" : 1
        }
    }
  }
  ```
- 수행 결과, counter 컬럼의 값이 1 증가함을 볼 수 있음
- 엘라스틱서치에서 제공하는 Update는 엄밀히 말하면 Update가 아니다.
- Update API가 호출되면 인덱스에서 문서를 가져와 스크립트를 수행한 후, 이를 다시 재색인(Reindex)
함
- 이러한 동작 원리 떄문에 Update API를 사용하기 위해서는 _source 필드가 활성화돼 있어야 함

### Bulk API
- 다수의 문서를 색인하거나 삭제 가능
- ```
  POST _bulk
  
  { "index": { "_index" : "인덱스명", "_type" : "_doc", "_id" : "1" } }
  { "title" : "살아남은 아이" }
  
  { "delete": { "_index" : "인덱스명", "_type" : "_doc", "_id" : "2" } }
  
  { "index": { "_index" : "인덱스명", "_type" : "_doc", "_id" : "3" } }
  { "title" : "프렌즈: 몬스터섬의 비밀" }
  
  { "update": { "_index" : "인덱스명", "_type" : "_doc", "_id" : "1" } }
  { "title" : "Last Child" }
  ```
- bulk API 단점 : 여러 건의 데이터가 한 번에 처리되기 때문에 도중에 실패가 발생하더라도 이미 갱신되거나 수정된 결과는 롤백되지 않음
- 그러므로 항상 처리결과를 확인해야함

### Reindex API
- 일반적으로 Reindex API를 사용하는 상황은 한 인덱스에서 다른 인덱스로 문서를 복사하는 경우임
- ```
  POST /_reindex
  {
    "source": {
      "index": "인덱스명"
    },
    "dest": {
      "index": "인덱스명_new"
    }
  }
  ```
- source가 북사할 인덱스
- dest가 복사될 인덱스
- 특정 조회 결과에 일치하는 문서만 복사할 수도 있음
  ```
  POST /_reindex
  {
    "source": {
      "index": "인덱스명",
      "type" : "_doc",
      "query": {
        "term": {
          "title.keyword": "프렌즈: 몬스터섬의비밀"
        }
      }
    },
    "dest": {
      "index": "인덱스명_new"
    }
  }
  ```
- 검색 시 정렬 작업은 리소스를 많이 사용하기 때문에 색인할 때 정렬된 상태로 색인할 수 있다면 좋음
- 기본적으로 Reindex API는 1,000건 단위로 스크롤을 수행함
- 이때 size 항목을 지정해 스크롤 크기를 변경할 수 있음
